{"cells":[{"cell_type":"markdown","source":["#ACCURACY TESTS\n","\n","Once the data is well divided and clean, the accuracy tests can take place. In this notebook we find the code for running each kind of test, divided by type of model (Naive Bayes, Support Vector Machines, Logistic Regression). There is no output in this notebook per se, but after running the code different .txt files with accuracy values will be found in an organised structure in Drive.\n","\n","*Note: When saving or loading data from Drive, the paths are specific to my personal Drive*"],"metadata":{"id":"N2EdVO0AVFgb"}},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3872,"status":"ok","timestamp":1661351805609,"user":{"displayName":"ELA KATHERINE SHEPHERD AREVALO","userId":"06530263046701412488"},"user_tz":-120},"id":"KnaiauIPXPgr","outputId":"e2f77c5c-96c6-4163-8f94-d836f11d7535"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (2.0.0)\n"]}],"source":["#Imports\n","!pip install emoji\n","import emoji\n","from sklearn.model_selection import train_test_split \n","from sklearn.utils import shuffle\n","from sklearn.metrics import classification_report, plot_confusion_matrix, accuracy_score\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n","from sklearn.linear_model import LogisticRegression\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import json\n","import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2232,"status":"ok","timestamp":1661351807830,"user":{"displayName":"ELA KATHERINE SHEPHERD AREVALO","userId":"06530263046701412488"},"user_tz":-120},"id":"guAPDPIOUL6e","outputId":"5c1f2294-b0eb-40d8-ee25-e0f5e5270091"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive \n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"NYLAfnz5HHpK"},"source":["#NAIVE BAYES\n"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":240,"status":"ok","timestamp":1661352132693,"user":{"displayName":"ELA KATHERINE SHEPHERD AREVALO","userId":"06530263046701412488"},"user_tz":-120},"id":"eZ5iNKM9XS7w"},"outputs":[],"source":["def naive_bayes(i, balance, lang, emo_hash, test, nbtype, vectorizer):\n","  if i == 0:\n","    i = 1\n","  if balance:\n","    #we save our txt files in a general path, I later save them into their specific folders\n","    path = \"/content/drive/MyDrive/TFG/data/accuracy_data/NB_\" + lang + \"_\" + emo_hash + \"_\" + str(100-(test*100)) + str(test*100) +\"TrainTest_\" + nbtype + \"_\" + vectorizer + \"_BALANCED.txt\"\n","  else:\n","    path = \"/content/drive/MyDrive/TFG/data/accuracy_data/NB_\" + lang + \"_\" + emo_hash + \"_\" + str(100-(test*100)) + str(test*100) +\"TrainTest_\" + nbtype + \"_\" + vectorizer + \"_NOT_BALANCED.txt\"\n","  for x in range(i):\n","    #Data loading---------------------------------GENERAL--CODE-----------------------------------------------\n","    df = pd.read_csv('/content/drive/MyDrive/TFG/data/final_data/' + emo_hash + '_' + lang + '_data.csv', encoding='utf8', engine='python')\n","    #Final row cleansing\n","    df = df[(df['hate speech'] == 0) | (df['hate speech'] == 1)]\n","    df = df.dropna()\n","    df = shuffle(df)\n","    #Balancing data\n","    if balance: \n","      pos_rows = len(df[df[\"hate speech\"] == True].index)\n","      fraction_to_delete = 1 - (pos_rows/ (df.shape[0]-pos_rows))\n","      df = df.drop(df[df['hate speech'] == 0].sample(frac=fraction_to_delete).index)\n","    #Train and test split\n","    X, y = df.text.fillna(' '), df[\"hate speech\"]\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test)\n","    #Vectorizing\n","    with open(\"/content/drive/MyDrive/TFG/data/stopwords/\" + lang + \"_stopwords.json\", \"r\") as f:\n","      json_text = f.read()\n","    stopwords = list(json.loads(json_text))\n","    if vectorizer == \"CountVectorizer\":\n","      vect = CountVectorizer(stop_words = stopwords, binary = True)\n","    else:\n","      vect = TfidfVectorizer(stop_words = stopwords, binary = True) # tfidf here\n","    X_train_vect = vect.fit_transform(X_train)\n","    X_test_vect = vect.transform(X_test)\n","    #print(len(vect.get_feature_names_out())) #code to calculate average vocabulary of train data\n","    #Model building---------------------------------GENERAL--CODE-----------------------------------------------\n","    if nbtype == \"Bernoulli\":\n","      model = BernoulliNB()\n","    else:\n","      model = MultinomialNB()\n","    model.fit(X_train_vect, y_train)\n","    #Result printing\n","    y_pred = model.predict(X_test_vect)\n","    acc_file = open(path,\"a\") \n","    acc_file.write(str(accuracy_score(y_test, y_pred) * 100) + \"\\n\")\n","    print(\"Accuracy score for Naive Bayes is: \", accuracy_score(y_test, y_pred) * 100, '%')\n","    acc_file.close()\n","  return model, vect"]},{"cell_type":"markdown","metadata":{"id":"DFWGClteHCOc"},"source":["#SUPPORT VECTOR MACHINES\n","Using SVM classifiers for text classification tasks might be a really good idea, especially if the training data available is not much (~ a couple of thousand tagged samples)."]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1661351807832,"user":{"displayName":"ELA KATHERINE SHEPHERD AREVALO","userId":"06530263046701412488"},"user_tz":-120},"id":"nPVaJ7veUb_m"},"outputs":[],"source":["def support_vector_machine(i, balance, lang, emo_hash, test, kernel, c, vectorizer):\n","  if i == 0:\n","    i = 1\n","  if balance:\n","    #we save our txt files in a general path, I later save them into their specific folders\n","    path = \"/content/drive/MyDrive/TFG/data/accuracy_data/SVM_\" + lang + \"_\" + emo_hash + \"_\" + str(100-(test*100)) + str(test*100) +\"TrainTest_\" + kernel + \"Kernel_\" + str(c) + \"C_\" + vectorizer + \"_BALANCED.txt\"\n","  else:\n","    path = \"/content/drive/MyDrive/TFG/data/accuracy_data/SVM_\" + lang + \"_\" + emo_hash + \"_\" + str(100-(test*100)) + str(test*100) +\"TrainTest_\" + kernel + \"Kernel_\" + str(c) + \"C_\" + vectorizer + \"_NOT_BALANCED.txt\"\n","  for x in range(i):\n","    #Data loading---------------------------------GENERAL--CODE-----------------------------------------------\n","    df = pd.read_csv('/content/drive/MyDrive/TFG/data/final_data/' + emo_hash + '_' + lang + '_data.csv', encoding='utf8', engine='python')\n","    #Final row cleansing\n","    df = df[(df['hate speech'] == 0) | (df['hate speech'] == 1)]\n","    df = df.dropna()\n","    df = shuffle(df)\n","    #Balancing data\n","    if balance: \n","      pos_rows = len(df[df[\"hate speech\"] == True].index)\n","      fraction_to_delete = 1 - (pos_rows/ (df.shape[0]-pos_rows))\n","      df = df.drop(df[df['hate speech'] == 0].sample(frac=fraction_to_delete).index)\n","    #Train and test split\n","    X, y = df.text.fillna(' '), df[\"hate speech\"]\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test)\n","    #Vectorizing\n","    with open(\"/content/drive/MyDrive/TFG/data/stopwords/\" + lang + \"_stopwords.json\", \"r\") as f:\n","      json_text = f.read()\n","    stopwords = list(json.loads(json_text))\n","    if vectorizer == \"CountVectorizer\":\n","      vect = CountVectorizer(stop_words = stopwords, binary = True)\n","    else:\n","      vect = TfidfVectorizer(stop_words = stopwords, binary = True) # tfidf here\n","    X_train_vect = vect.fit_transform(X_train)\n","    X_test_vect = vect.transform(X_test)\n","    #Model building---------------------------------GENERAL--CODE-----------------------------------------------\n","    model = SVC(kernel=kernel, C=c)\n","    model.fit(X_train_vect, y_train)\n","    #Result printing\n","    y_pred = model.predict(X_test_vect)\n","    acc_file = open(path,\"a\") \n","    acc_file.write(str(accuracy_score(y_test, y_pred) * 100) + \"\\n\")\n","    print(\"Accuracy score for SVC is: \", accuracy_score(y_test, y_pred) * 100, '%')\n","    acc_file.close()\n","  return model, vect"]},{"cell_type":"markdown","metadata":{"id":"iXV_qvLYHFZQ"},"source":["#LOGISTIC REGRESSION\n"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1661351807833,"user":{"displayName":"ELA KATHERINE SHEPHERD AREVALO","userId":"06530263046701412488"},"user_tz":-120},"id":"hK4wM-TxQBFy"},"outputs":[],"source":["def logistic_regression(i, balance, lang, emo_hash, test, solver, c, vectorizer):\n","  if i == 0:\n","    i = 1\n","  if balance:\n","    #we save our txt files in a general path, I later save them into their specific folders\n","    path = \"/content/drive/MyDrive/TFG/data/accuracy_data/LR_\" + lang + \"_\" + emo_hash + \"_\" + str(100-(test*100)) + str(test*100) +\"TrainTest_\" + solver + \"Solver_\" + str(c) + \"C_\" + vectorizer + \"_BALANCED.txt\"\n","  else:\n","    path = \"/content/drive/MyDrive/TFG/data/accuracy_data/LR_\" + lang + \"_\" + emo_hash + \"_\" + str(100-(test*100)) + str(test*100) +\"TrainTest_\" + solver + \"Solver_\" + str(c) + \"C_\" + vectorizer + \"_NOT_BALANCED.txt\"\n","  #\"w\" to write, \"a\" to append\n","  for x in range(i):\n","    #Data loading---------------------------------GENERAL--CODE-----------------------------------------------\n","    df = pd.read_csv('/content/drive/MyDrive/TFG/data/final_data/' + emo_hash + '_' + lang + '_data.csv', encoding='utf8', engine='python')\n","    #Final row cleansing\n","    df = df[(df['hate speech'] == 0) | (df['hate speech'] == 1)]\n","    df = df.dropna()\n","    df = shuffle(df)\n","    #Balancing data\n","    if balance: \n","      pos_rows = len(df[df[\"hate speech\"] == True].index)\n","      fraction_to_delete = 1 - (pos_rows/ (df.shape[0]-pos_rows))\n","      df = df.drop(df[df['hate speech'] == 0].sample(frac=fraction_to_delete).index)\n","    #Train and test split\n","    X, y = df.text.fillna(' '), df[\"hate speech\"]\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test)\n","    #Vectorizing\n","    with open(\"/content/drive/MyDrive/TFG/data/stopwords/\" + lang + \"_stopwords.json\", \"r\") as f:\n","      json_text = f.read()\n","    stopwords = list(json.loads(json_text))\n","    if vectorizer == \"CountVectorizer\":\n","      vect = CountVectorizer(stop_words = stopwords, binary = True)\n","    else:\n","      vect = TfidfVectorizer(stop_words = stopwords, binary = True) # tfidf here\n","    X_train_vect = vect.fit_transform(X_train)\n","    X_test_vect = vect.transform(X_test)\n","    #Model building---------------------------------GENERAL--CODE-----------------------------------------------\n","    model = LogisticRegression(solver=solver, C=c)\n","    model.fit(X_train_vect, y_train)\n","    #Result printing\n","    y_pred = model.predict(X_test_vect)\n","    acc_file = open(path,\"a\") \n","    acc_file.write(str(accuracy_score(y_test, y_pred) * 100) + \"\\n\")\n","    print(\"Accuracy score for Logistic Regression is: \", accuracy_score(y_test, y_pred) * 100, '%')\n","    acc_file.close()\n","  return model, vect"]},{"cell_type":"markdown","metadata":{"id":"EaBLHdk3y0ts"},"source":["------------------------------------------------------------------------\n","#Accuracy tests"]},{"cell_type":"code","execution_count":55,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"sdllLCBZSd8o","executionInfo":{"status":"ok","timestamp":1661353986501,"user_tz":-120,"elapsed":42980,"user":{"displayName":"ELA KATHERINE SHEPHERD AREVALO","userId":"06530263046701412488"}},"outputId":"7a03d4f7-0538-4510-9c37-184391b13cb6"},"outputs":[{"output_type":"stream","name":"stdout","text":["ENGLISH DATA WITH ALL EMOJIS AND HASHTAGS USING MODEL MULTINOMIAL NB, WITH TF-IDF AND TRAIN/TEST SPLIT OF 70/30\n","-------------------------------------------------------------------------------------------------------------------------------\n","30110\n","30297\n","30028\n","30372\n","30211\n","29956\n","30425\n","30315\n","30175\n","30273\n","30169\n","30156\n","30025\n","30227\n","30109\n","30432\n","30023\n","30276\n","30103\n","30340\n","29998\n","30173\n","30332\n","30199\n","30483\n","30334\n","30101\n","30050\n","30366\n","30105\n"]}],"source":["#@title 1. Choose a model and language\n","\n","Language = 'English'  #@param [\"Spanish\", \"Italian\", \"Portuguese\", \"English\"]\n","\n","map_lang_data = {\n","    'Spanish':\n","        'spanish',\n","    'Italian':\n","        'italian',\n","    'Portuguese':\n","        'portuguese',\n","    'English':\n","        'english',\n","}\n","\n","Emojis_Hashtags = 'All emojis and hashtags'  #@param [\"All emojis and hashtags\", \"No emojis or hashtags\"]\n","\n","map_emojhash_data = {\n","    'All emojis and hashtags':\n","        'mantained',\n","    'No emojis or hashtags':\n","        'removed',\n","}\n","\n","Train_Test_Split = '70/30'  #@param ['60/40', '70/30', '80/20']\n","\n","map_test_split = {\n","    '60/40':\n","        0.4,\n","    '70/30':\n","        0.3,\n","    '80/20':\n","        0.2,\n","}\n","\n","Model = 'Multinomial NB'  #@param [\"Bernoulli NB\", \"Multinomial NB\", \"SVM linear kernel small C\", \"SVM linear kernel standard C\", \"SVM linear kernel large C\", \"SVM RBF kernel small C\", \"SVM RBF kernel standard C\", \"SVM RBF kernel large C\", \"LR liblinear solver small C\", \"LR liblinear solver standard C\",\"LR liblinear solver large C\", \"LR lbfgs solver small C\", \"LR lbfgs solver standard C\", \"LR lbfgs solver large C\"]\n","\n","TF_IDF = True #@param {type:\"boolean\"}\n","Test_Iterations = 30 #@param {type:\"slider\", min:1, max:50, step:1}\n","Balance_data = True #@param {type:\"boolean\"}\n","\n","map_vectorizer = {\n","    True:\n","        \"TfidfVectorizer\",\n","    False:\n","        \"CountVectorizer\",\n","}\n","\n","lang = map_lang_data[Language]\n","emo_hash = map_emojhash_data[Emojis_Hashtags]\n","test = map_test_split[Train_Test_Split]\n","vectorizer = map_vectorizer[TF_IDF]\n","if TF_IDF:\n","  vectorizer_word = \"with\"\n","else:\n","  vectorizer_word = \"without\"\n","\n","print((Language + \" data with \" + Emojis_Hashtags +\" using model \" + Model + \", \" + vectorizer_word + \" TF-IDF and train/test split of \" + Train_Test_Split).upper())\n","print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n","\n","if Model == \"SVM linear kernel small C\":\n","  chosen_model, vect = support_vector_machine(Test_Iterations, Balance_data, lang, emo_hash, test, \"linear\", 0.1, vectorizer)\n","elif Model == \"SVM linear kernel standard C\":\n","  chosen_model, vect = support_vector_machine(Test_Iterations, Balance_data, lang, emo_hash, test, \"linear\", 1, vectorizer)\n","elif Model == \"SVM linear kernel large C\":\n","  chosen_model, vect = support_vector_machine(Test_Iterations, Balance_data, lang, emo_hash, test, \"linear\", 10, vectorizer)\n","elif Model == \"SVM RBF kernel small C\":\n","  chosen_model, vect = support_vector_machine(Test_Iterations, Balance_data, lang, emo_hash, test, \"rbf\", 0.1, vectorizer)\n","elif Model == \"SVM RBF kernel standard C\":\n","  chosen_model, vect = support_vector_machine(Test_Iterations, Balance_data, lang, emo_hash, test, \"rbf\", 1, vectorizer)\n","elif Model == \"SVM RBF kernel large C\":\n","  chosen_model, vect = support_vector_machine(Test_Iterations, Balance_data, lang, emo_hash, test, \"rbf\", 10, vectorizer)\n","\n","elif Model == \"Bernoulli NB\":\n","  chosen_model, vect = naive_bayes(Test_Iterations, Balance_data, lang, emo_hash, test, \"Bernoulli\", vectorizer)\n","elif Model == \"Multinomial NB\":\n","  chosen_model, vect = naive_bayes(Test_Iterations, Balance_data, lang, emo_hash, test, \"Multinomial\", vectorizer)\n","\n","elif Model == \"LR liblinear solver small C\":\n","  chosen_model, vect = logistic_regression(Test_Iterations, Balance_data, lang, emo_hash, test, \"liblinear\", 0.1, vectorizer)\n","elif Model == \"LR liblinear solver standard C\":\n","  chosen_model, vect = logistic_regression(Test_Iterations, Balance_data, lang, emo_hash, test, \"liblinear\", 1, vectorizer)\n","elif Model == \"LR liblinear solver large C\":\n","  chosen_model, vect = logistic_regression(Test_Iterations, Balance_data, lang, emo_hash, test, \"liblinear\", 10, vectorizer)\n","elif Model == \"LR lbfgs solver small C\":\n","  chosen_model, vect = logistic_regression(Test_Iterations, Balance_data, lang, emo_hash, test, \"lbfgs\", 0.1, vectorizer)\n","elif Model == \"LR lbfgs solver standard C\":\n","  chosen_model, vect = logistic_regression(Test_Iterations, Balance_data, lang, emo_hash, test, \"lbfgs\", 1, vectorizer)\n","elif Model == \"LR lbfgs solver large C\":\n","  chosen_model, vect = logistic_regression(Test_Iterations, Balance_data, lang, emo_hash, test, \"lbfgs\", 10, vectorizer)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"ML_accuracy_tests.ipynb","provenance":[],"authorship_tag":"ABX9TyMM8fFTz/C6LseHm5Y4Cxz8"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
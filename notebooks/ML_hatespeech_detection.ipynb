{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#HATE SPEECH DETECTION\n",
        "\n",
        "In this notebook, a user could select what kind of Machine Learning algorithm to train and the data that will be used to train it; before trying out the model by writing text and seeing if it contains hate speech or not according to the AI.\n",
        "\n",
        "The notebook is similar in code to \"ML_accuracy_tests.ipynb\", but without the option to train the test more than once and with a possibility of printing a classification report and confusion matrix when training a chosen algorithm.\n",
        "\n",
        "*Note: When saving or loading data from Drive, the paths are specific to my personal Drive*"
      ],
      "metadata": {
        "id": "qy1iCLaXfhOC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnaiauIPXPgr",
        "outputId": "58875bf8-19e6-4d2d-e02d-4780133c5e11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.0.0.tar.gz (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 5.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.0.0-py3-none-any.whl size=193022 sha256=c43778d280bb8c78315d65bf781249e67f472342b0c032f25a97362a5ae76742\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/29/4d/3cfe7452ac7d8d83b1930f8a6205c3c9649b24e80f9029fc38\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.0.0\n"
          ]
        }
      ],
      "source": [
        "#Imports\n",
        "!pip install emoji\n",
        "import emoji\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import classification_report, plot_confusion_matrix, accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guAPDPIOUL6e",
        "outputId": "85018e69-c58c-4604-8815-86ae01153f7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYLAfnz5HHpK"
      },
      "source": [
        "#NAIVE BAYES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ5iNKM9XS7w"
      },
      "outputs": [],
      "source": [
        "def naive_bayes(balance, lang, emo_hash, test, nbtype, print_metrics, vectorizer):\n",
        "  #Data loading---------------------------------GENERAL--CODE-----------------------------------------------\n",
        "  df = pd.read_csv('/content/drive/MyDrive/TFG/data/final_data/' + emo_hash + '_' + lang + '_data.csv', encoding='utf8', engine='python')\n",
        "  #Final row cleansing\n",
        "  df = df[(df['hate speech'] == 0) | (df['hate speech'] == 1)]\n",
        "  df = df.dropna()\n",
        "  df = shuffle(df)\n",
        "  #Balancing data\n",
        "  if balance: \n",
        "    pos_rows = len(df[df[\"hate speech\"] == True].index)\n",
        "    fraction_to_delete = 1 - (pos_rows/ (df.shape[0]-pos_rows))\n",
        "    df = df.drop(df[df['hate speech'] == 0].sample(frac=fraction_to_delete).index)\n",
        "  #Train and test split\n",
        "  X, y = df.text.fillna(' '), df[\"hate speech\"]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test)\n",
        "  #Vectorizing\n",
        "  with open(\"/content/drive/MyDrive/TFG/data/stopwords/\" + lang + \"_stopwords.json\", \"r\") as f:\n",
        "    json_text = f.read()\n",
        "  stopwords = list(json.loads(json_text))\n",
        "  if vectorizer == \"CountVectorizer\":\n",
        "    vect = CountVectorizer(stop_words = stopwords, binary = True)\n",
        "  else:\n",
        "    vect = TfidfVectorizer(stop_words = stopwords, binary = True) # tfidf here\n",
        "  X_train_vect = vect.fit_transform(X_train)\n",
        "  X_test_vect = vect.transform(X_test)\n",
        "  #Model building---------------------------------GENERAL--CODE-----------------------------------------------\n",
        "  if nbtype == \"Bernoulli\":\n",
        "    model = BernoulliNB()\n",
        "  else:\n",
        "    model = MultinomialNB()\n",
        "  model.fit(X_train_vect, y_train)\n",
        "  #Plot printing\n",
        "  if print_metrics:\n",
        "    y_pred = model.predict(X_test_vect)\n",
        "    print(classification_report(y_test,y_pred))\n",
        "    print(\"-------------------------------------------------------\")\n",
        "    print(\"Accuracy score for Naive Bayes is: \", accuracy_score(y_test, y_pred) * 100, '%')\n",
        "    print(\"-------------------------------------------------------\")\n",
        "    #cmap options: YlOrRd, GnBu, RdPu, YlGn\n",
        "    plot_confusion_matrix(model, X_test_vect, y_test, normalize='true', cmap =\"GnBu\")\n",
        "  return model, vect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFWGClteHCOc"
      },
      "source": [
        "#SUPPORT VECTOR MACHINES\n",
        "Using SVM classifiers for text classification tasks might be a really good idea, especially if the training data available is not much (~ a couple of thousand tagged samples)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPVaJ7veUb_m"
      },
      "outputs": [],
      "source": [
        "def support_vector_machine(balance, lang, emo_hash, test, kernel, c, print_metrics, vectorizer):\n",
        "  #Data loading---------------------------------GENERAL--CODE-----------------------------------------------\n",
        "  df = pd.read_csv('/content/drive/MyDrive/TFG/data/final_data/' + emo_hash + '_' + lang + '_data.csv', encoding='utf8', engine='python')\n",
        "  #Final row cleansing\n",
        "  df = df[(df['hate speech'] == 0) | (df['hate speech'] == 1)]\n",
        "  df = df.dropna()\n",
        "  df = shuffle(df)\n",
        "  #Balancing data\n",
        "  if balance: \n",
        "    pos_rows = len(df[df[\"hate speech\"] == True].index)\n",
        "    fraction_to_delete = 1 - (pos_rows/ (df.shape[0]-pos_rows))\n",
        "    df = df.drop(df[df['hate speech'] == 0].sample(frac=fraction_to_delete).index)\n",
        "  #Train and test split\n",
        "  X, y = df.text.fillna(' '), df[\"hate speech\"]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test)\n",
        "  #Vectorizing\n",
        "  with open(\"/content/drive/MyDrive/TFG/data/stopwords/\" + lang + \"_stopwords.json\", \"r\") as f:\n",
        "    json_text = f.read()\n",
        "  stopwords = list(json.loads(json_text))\n",
        "  if vectorizer == \"CountVectorizer\":\n",
        "    vect = CountVectorizer(stop_words = stopwords, binary = True)\n",
        "  else:\n",
        "    vect = TfidfVectorizer(stop_words = stopwords, binary = True) # tfidf here\n",
        "  X_train_vect = vect.fit_transform(X_train)\n",
        "  X_test_vect = vect.transform(X_test)\n",
        "  #Model building---------------------------------GENERAL--CODE-----------------------------------------------\n",
        "  model = SVC(kernel=kernel, C=c)\n",
        "  model.fit(X_train_vect, y_train)\n",
        "  #Plot printing\n",
        "  if print_metrics:\n",
        "    y_pred = model.predict(X_test_vect)\n",
        "    print(classification_report(y_test,y_pred))\n",
        "    print(\"-------------------------------------------------------\")\n",
        "    print(\"Accuracy score for SVC is: \", accuracy_score(y_test, y_pred) * 100, '%')\n",
        "    print(\"-------------------------------------------------------\")\n",
        "    #cmap options: YlOrRd, GnBu, RdPu, YlGn\n",
        "    plot_confusion_matrix(model, X_test_vect, y_test, normalize='true', cmap =\"YlGn\")\n",
        "  return model, vect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXV_qvLYHFZQ"
      },
      "source": [
        "#LOGISTIC REGRESSION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK4wM-TxQBFy"
      },
      "outputs": [],
      "source": [
        "def logistic_regression(balance, lang, emo_hash, test, solver, c, print_metrics, vectorizer):\n",
        "  #Data loading---------------------------------GENERAL--CODE-----------------------------------------------\n",
        "  df = pd.read_csv('/content/drive/MyDrive/TFG/data/final_data/' + emo_hash + '_' + lang + '_data.csv', encoding='utf8', engine='python')\n",
        "  #Final row cleansing\n",
        "  df = df[(df['hate speech'] == 0) | (df['hate speech'] == 1)]\n",
        "  df = df.dropna()\n",
        "  df = shuffle(df)\n",
        "  #Balancing data\n",
        "  if balance: \n",
        "    pos_rows = len(df[df[\"hate speech\"] == True].index)\n",
        "    fraction_to_delete = 1 - (pos_rows/ (df.shape[0]-pos_rows))\n",
        "    df = df.drop(df[df['hate speech'] == 0].sample(frac=fraction_to_delete).index)\n",
        "  #Train and test split\n",
        "  X, y = df.text.fillna(' '), df[\"hate speech\"]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test)\n",
        "  #Vectorizing\n",
        "  with open(\"/content/drive/MyDrive/TFG/data/stopwords/\" + lang + \"_stopwords.json\", \"r\") as f:\n",
        "    json_text = f.read()\n",
        "  stopwords = list(json.loads(json_text))\n",
        "  if vectorizer == \"CountVectorizer\":\n",
        "    vect = CountVectorizer(stop_words = stopwords, binary = True)\n",
        "  else:\n",
        "    vect = TfidfVectorizer(stop_words = stopwords, binary = True) # tfidf here\n",
        "  X_train_vect = vect.fit_transform(X_train)\n",
        "  X_test_vect = vect.transform(X_test)\n",
        "  #Model building---------------------------------GENERAL--CODE-----------------------------------------------\n",
        "  model = LogisticRegression(solver=solver, C=c)\n",
        "  model.fit(X_train_vect, y_train)\n",
        "  #Plot printing\n",
        "  if print_metrics:\n",
        "    y_pred = model.predict(X_test_vect)\n",
        "    print(classification_report(y_test,y_pred))\n",
        "    print(\"-------------------------------------------------------\")\n",
        "    print(\"Accuracy score for Logistic Regression is: \", accuracy_score(y_test, y_pred) * 100, '%')\n",
        "    print(\"-------------------------------------------------------\")\n",
        "    #cmap options: YlOrRd, GnBu, RdPu, YlGn\n",
        "    plot_confusion_matrix(model, X_test_vect, y_test, normalize='true', cmap =\"RdPu\")\n",
        "  return model, vect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaBLHdk3y0ts"
      },
      "source": [
        "------------------------------------------------------------------------\n",
        "#Accuracy checker and hate speech detectors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*RECOMMENDED MODEL SETTINGS FOR BEST RESULTS:*\n",
        "****\n",
        "**For all languages:** All emojis and hashtags, Balance_data ✅\n",
        "\n",
        "**English:** 80/20 train/test split, SVM RBF kernel standard C model, TF_IDF ❌\n",
        "\n",
        "**Spanish:** 80/20 train/test split, SVM RBF kernel standard C model, TF_IDF ✅\n",
        "\n",
        "**Italian:** 80/20 train/test split, SVM RBF kernel large C model, TF_IDF ✅\n",
        "\n",
        "**Portuguese:** 60/40 train/test split, Multinomial NB, TF_IDF ✅\n",
        "\n",
        "```\n",
        "Notes:\n",
        "  a. Italian models obtain best results\n",
        "  b. Portuguese models obtain worst results\n",
        "  c. English models' execution time may be long\n",
        "```\n"
      ],
      "metadata": {
        "id": "BrbbhBnbdOmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Choose a model and language\n",
        "\n",
        "Language = 'English'  #@param [\"Spanish\", \"Italian\", \"Portuguese\", \"English\"]\n",
        "\n",
        "map_lang_data = {\n",
        "    'Spanish':\n",
        "        'spanish',\n",
        "    'Italian':\n",
        "        'italian',\n",
        "    'Portuguese':\n",
        "        'portuguese',\n",
        "    'English':\n",
        "        'english',\n",
        "}\n",
        "\n",
        "Emojis_Hashtags = 'All emojis and hashtags'  #@param [\"All emojis and hashtags\", \"No emojis or hashtags\"]\n",
        "\n",
        "map_emojhash_data = {\n",
        "    'All emojis and hashtags':\n",
        "        'mantained',\n",
        "    'No emojis or hashtags':\n",
        "        'removed',\n",
        "}\n",
        "\n",
        "Train_Test_Split = '80/20'  #@param ['60/40', '70/30', '80/20']\n",
        "\n",
        "map_test_split = {\n",
        "    '60/40':\n",
        "        0.4,\n",
        "    '70/30':\n",
        "        0.3,\n",
        "    '80/20':\n",
        "        0.2,\n",
        "}\n",
        "\n",
        "Model = 'SVM RBF kernel standard C'  #@param [\"Bernoulli NB\", \"Multinomial NB\", \"SVM linear kernel small C\", \"SVM linear kernel standard C\", \"SVM linear kernel large C\", \"SVM RBF kernel small C\", \"SVM RBF kernel standard C\", \"SVM RBF kernel large C\", \"LR liblinear solver small C\", \"LR liblinear solver standard C\",\"LR liblinear solver large C\", \"LR lbfgs solver small C\", \"LR lbfgs solver standard C\", \"LR lbfgs solver large C\"]\n",
        "\n",
        "TF_IDF = True #@param {type:\"boolean\"}\n",
        "Print_model_metrics = True #@param {type:\"boolean\"}\n",
        "Balance_data = True #@param {type:\"boolean\"}\n",
        "\n",
        "map_vectorizer = {\n",
        "    True:\n",
        "        \"TfidfVectorizer\",\n",
        "    False:\n",
        "        \"CountVectorizer\",\n",
        "}\n",
        "\n",
        "map_balanced = {\n",
        "    True:\n",
        "        \"BALANCED\",\n",
        "    False:\n",
        "        \"NON BALANCED\",\n",
        "}\n",
        "\n",
        "lang = map_lang_data[Language]\n",
        "emo_hash = map_emojhash_data[Emojis_Hashtags]\n",
        "test = map_test_split[Train_Test_Split]\n",
        "vectorizer = map_vectorizer[TF_IDF]\n",
        "balanced = map_balanced[Balance_data]\n",
        "\n",
        "if TF_IDF:\n",
        "  vectorizer_word = \"with\"\n",
        "else:\n",
        "  vectorizer_word = \"without\"\n",
        "\n",
        "print((Language + \" data with \" + Emojis_Hashtags +\" using model \" + Model + \", \" + vectorizer_word + \" TF-IDF and train/test split of \" + Train_Test_Split).upper() + \", \" + balanced + \" DATA\")\n",
        "print(\"-------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "if Model == \"SVM linear kernel small C\":\n",
        "  chosen_model, vect = support_vector_machine(Balance_data, lang, emo_hash, test, \"linear\", 0.1, Print_model_metrics, vectorizer)\n",
        "elif Model == \"SVM linear kernel standard C\":\n",
        "  chosen_model, vect = support_vector_machine(Balance_data, lang, emo_hash, test, \"linear\", 1, Print_model_metrics, vectorizer)\n",
        "elif Model == \"SVM linear kernel large C\":\n",
        "  chosen_model, vect = support_vector_machine(Balance_data, lang, emo_hash, test, \"linear\", 10, Print_model_metrics, vectorizer)\n",
        "elif Model == \"SVM RBF kernel small C\":\n",
        "  chosen_model, vect = support_vector_machine(Balance_data, lang, emo_hash, test, \"rbf\", 0.1, Print_model_metrics, vectorizer)\n",
        "elif Model == \"SVM RBF kernel standard C\":\n",
        "  chosen_model, vect = support_vector_machine(Balance_data, lang, emo_hash, test, \"rbf\", 1, Print_model_metrics, vectorizer)\n",
        "elif Model == \"SVM RBF kernel large C\":\n",
        "  chosen_model, vect = support_vector_machine(Balance_data, lang, emo_hash, test, \"rbf\", 10, Print_model_metrics, vectorizer)\n",
        "\n",
        "elif Model == \"Bernoulli NB\":\n",
        "  chosen_model, vect = naive_bayes(Balance_data, lang, emo_hash, test, \"Bernoulli\", Print_model_metrics, vectorizer)\n",
        "elif Model == \"Multinomial NB\":\n",
        "  chosen_model, vect = naive_bayes(Balance_data, lang, emo_hash, test, \"Multinomial\", Print_model_metrics, vectorizer)\n",
        "\n",
        "elif Model == \"LR liblinear solver small C\":\n",
        "  chosen_model, vect = logistic_regression(Balance_data, lang, emo_hash, test, \"liblinear\", 0.1, Print_model_metrics, vectorizer)\n",
        "elif Model == \"LR liblinear solver standard C\":\n",
        "  chosen_model, vect = logistic_regression(Balance_data, lang, emo_hash, test, \"liblinear\", 1, Print_model_metrics, vectorizer)\n",
        "elif Model == \"LR liblinear solver large C\":\n",
        "  chosen_model, vect = logistic_regression(Balance_data, lang, emo_hash, test, \"liblinear\", 10, Print_model_metrics, vectorizer)\n",
        "elif Model == \"LR lbfgs solver small C\":\n",
        "  chosen_model, vect = logistic_regression(Balance_data, lang, emo_hash, test, \"lbfgs\", 0.1, Print_model_metrics, vectorizer)\n",
        "elif Model == \"LR lbfgs solver standard C\":\n",
        "  chosen_model, vect = logistic_regression(Balance_data, lang, emo_hash, test, \"lbfgs\", 1, Print_model_metrics, vectorizer)\n",
        "elif Model == \"LR lbfgs solver large C\":\n",
        "  chosen_model, vect = logistic_regression(Balance_data, lang, emo_hash, test, \"lbfgs\", 10, Print_model_metrics, vectorizer)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kQm3aQaxdGT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Write some text to see if it contains hate speech...\n",
        "text = \"\" #@param {type:\"string\"}\n",
        "text = text.encode('utf-16','surrogatepass').decode('utf-16')\n",
        "#demojize possible emojis\n",
        "emo_full_list = ''.join(c for c in text if c in emoji.UNICODE_EMOJI['en'])\n",
        "emo_unique_list = np.unique(list(emo_full_list))\n",
        "for k in emo_unique_list:\n",
        "    text = text.replace(k, \" \" + emoji.demojize(k) + \" \")\n",
        "\n",
        "prediction = chosen_model.predict(vect.transform([text]))\n",
        "\n",
        "if prediction == 1 or prediction == \"hate speech\":\n",
        "  print(\"YES, This text contains hate speech\")\n",
        "else:\n",
        "  print(\"NO, This text does not contain hate speech\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "u2P1H3iRdGgG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "NYLAfnz5HHpK",
        "DFWGClteHCOc",
        "iXV_qvLYHFZQ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}